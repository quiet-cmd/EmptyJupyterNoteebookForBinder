{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Загрузка зависимостей\n",
    "import numpy\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "from statsmodels.graphics.gofplots import qqplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка подготовленного набора данных\n",
    "dataset = pandas.read_csv('prepared_data.csv') # Убедиться в правильности пути к файлу!\n",
    "dataset.head(10) # Вывод первых 10 строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделение входных и выходных параметров:\n",
    "# Выходной параметр - значение столбца обработанного набора данных, соответсвующее столбцу №8 из первоначального набора данных (machine.data.csv).\n",
    "# Входные параметры - все остальные столбцы обработанного набора данных.\n",
    "# Используем .values, чтобы индексы не попали в новые переменные и в дальнейшем не мешали нам, например, при построении графиков.\n",
    "X = dataset.iloc[:, 1:37].values\n",
    "Y = dataset['8'].values\n",
    "\n",
    "# Обратите внимание на структуру вашего обработанного файлы и измените индексы, если нужно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение данных на обучающую и тестовую выборки:\n",
    "# Используем метод train_test_split библиотеки sklearn.\n",
    "# Он разделяет заданные массивы входных и выходных параметров на две части каждый.\n",
    "# Значение test_size определяет долю тестовой выборки в исходных данных.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задаем параметры структуры нейронной сети.\n",
    "\n",
    "# Количество нейронов во входном слое должно быть равно количеству входных параметров.\n",
    "input_layer_size = 36\n",
    "\n",
    "# Количество нейронов в каждом скрытом слое и количество таких слоёв могут быть выбраны произвольно.\n",
    "# Увеличение числа нейронов относительно предыдущего слоя позволяет лучше обрабатывать отдельные примеры.\n",
    "# Уменьшение числа нейронов относительно предыдущего слоя позволяет обощать информацию и выводить новые правила.\n",
    "# Увеличение числа слоёв помогает сети работать как более сложная функция.\n",
    "first_hidden_layer_size = 50\n",
    "second_hidden_layer_size = 50\n",
    "\n",
    "# Количество нейронов в выходном слое должно быть равно количеству выходных параметров.\n",
    "output_layer_size = 1\n",
    "\n",
    "# Параметры обучения нейронной сети: количество эпох и размер батча.\n",
    "epochs_number = 100\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Создание нейронной сети прямого распространения, пока она пустая, т.е. не содержит слоёв и нейронов.\n",
    "model = Sequential()\n",
    "\n",
    "# Входной слой и первый скрытый слой, функция активации - ReLU\n",
    "model.add(Dense(first_hidden_layer_size, input_dim=input_layer_size, activation='relu'))\n",
    "\n",
    "# Второй скрытый слой, функция активации - ReLU\n",
    "model.add(Dense(second_hidden_layer_size, activation='relu'))\n",
    "\n",
    "# Выходной слой, функция активации - линейная, т.к. решается задача регрессии.\n",
    "model.add(Dense(output_layer_size, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Обзор нейронной сети, теперь в ней есть слои и нейроны.\n",
    "# Если внимательно посмотреть на количество параметров, т.е. связей между нейронами, то можно увидеть, что автоматически добавлены нейроны смещения.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Настройка нейронной сети.\n",
    "# Функция потерь (loss) - среднеквадратическая ошибка.\n",
    "# Оптимизатор (optimizer) - adam (в настоящий момент - это стандартный оптимизатор).\n",
    "# Указываем метрики средняя абсолютная ошибка (mean_absolute_error) и среднеквадратическая ошиба (mean_squared_error) для дальнейшего анализа.\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_absolute_error', 'mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Обучение нейронной сети.\n",
    "# На вход передаем обучающие и тестовые выборки, количество эпох и размер батча.\n",
    "# На выходе получаем объект с историей обучения, который далее будем анализировать.\n",
    "history = model.fit(X_train, Y_train, validation_data = (X_test,Y_test), epochs=epochs_number, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выводим динамику среднего абсолютного отклонения от номера эпохи обучения.\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.plot(history.history['val_mean_absolute_error']) \n",
    "plt.title('Model MAE') \n",
    "plt.ylabel('MAE') \n",
    "plt.xlabel('Epoch') \n",
    "plt.legend(['Train', 'Test'], loc='upper right') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выводим динамику среднеквадратического отклонения, т.е. значения функции потерь, от номера эпохи обучения.\n",
    "# Видно, что в процессе обучения сеть с каждой эпохой вычисляет всё более точные прогнозы значений Y.\n",
    "plt.plot(history.history['mean_squared_error'])\n",
    "plt.plot(history.history['val_mean_squared_error']) \n",
    "plt.title('Model MSE') \n",
    "plt.ylabel('MSE') \n",
    "plt.xlabel('Epoch') \n",
    "plt.legend(['Train', 'Test'], loc='upper right') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказание уже обученной нейронной сети на обучающей выборке:\n",
    "Y_pred_train = model.predict(X_train).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравним эталонные значения Y_train и результат работы обученной нейронной сети Y_pred_train для обучающей выборки.\n",
    "# В идеальной ситуации они должны совпадать, т.е. точки (Y_train[i], Y_pred_train[i]) должны лежать на прямой Y_train = Y_pred_train.\n",
    "plt.plot(Y_train, Y_pred_train, 'bo')\n",
    "plt.plot([0,1], [0,1], 'r-')\n",
    "plt.title('Test vs Pred_test') \n",
    "plt.ylabel('Pred_test') \n",
    "plt.xlabel('Test') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выведем сами значения Y_train и Y_pred_train.\n",
    "plt.plot(Y_train)\n",
    "plt.plot(Y_pred_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Таким образом, обучение по обучающей выборке проведено.\n",
    "# Теперь проверим работу нейроной сети на тестовой выборке.\n",
    "\n",
    "# Предсказание обученной нейронной сети на тестовой выборке:\n",
    "Y_pred_test = model.predict(X_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Сравним эталонные значения Y_test и результат работы обученной нейронной сети Y_pred_test для тестовой выборки.\n",
    "plt.plot(Y_test, Y_pred_test, 'bo')\n",
    "plt.plot([0,1], [0,1], 'r-')\n",
    "plt.title('Test vs Pred_test') \n",
    "plt.ylabel('Pred_test') \n",
    "plt.xlabel('Test') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Выведем сами значения Y_test и Y_pred_test.\n",
    "plt.plot(Y_test)\n",
    "plt.plot(Y_pred_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Итого, качество работы нейронной сети на тестовых данных ниже, чем на обучающих - это ожидаемо.\n",
    "# Тем не менее, нейронная сеть способна найти зависимости и предсказвать значение Y по X с приемлемым уровенм точности.\n",
    "\n",
    "# Сравним среднеквадратичные ошибки (значения функции потерь) для обучающей и тестовой выборок.\n",
    "print(numpy.sqrt(mean_squared_error(Y_train, Y_pred_train)))\n",
    "print(numpy.sqrt(mean_squared_error(Y_test, Y_pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверим на нормальное распределение разности пар (Y_train, Y_pred_train), (Y_test, Y_pred_test)\n",
    "# Для этого используем библиотеку scipy и метод stats.shapiro.\n",
    "k_train, p_train = stats.shapiro(Y_train - Y_pred_train)\n",
    "print('Train k = {0}, p = {1}'.format(k_train, p_train))\n",
    "\n",
    "k_test, p_test = stats.shapiro(Y_test - Y_pred_test)\n",
    "print('Test k = {0}, p = {1}'.format(k_test, p_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для полной выборки (Y, Y_pred) применим два статистических теста: shapiro и normaltest.\n",
    "Y_pred = model.predict(X).flatten()\n",
    "\n",
    "k_s, p_s = stats.shapiro(Y - Y_pred)\n",
    "print('k_s = {0}, p_s = {1}'.format(k_s, p_s))\n",
    "\n",
    "k_n, p_n = stats.normaltest(Y - Y_pred)\n",
    "print('k_n = {0}, p_n = {1}'.format(k_n, p_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# И тоже самое визуально, с помощью грфиков квантиль-квантиль.\n",
    "# Обучающая выборка\n",
    "qqplot(Y_train - Y_pred_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Тестовая выборка\n",
    "qqplot(Y_test - Y_pred_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Полная выборка\n",
    "qqplot(Y - Y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Можо также визульно убедиться, что распределения \"почти нормальное\".\n",
    "plt.hist(Y - Y_pred, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраним обученную нейронную сеть, название файла можно выбрать любое, расширение - h5.\n",
    "model.save('SimpleNeuralNetwork.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
